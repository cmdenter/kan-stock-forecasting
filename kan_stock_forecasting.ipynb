{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KAN-based Autoregressive Model for Stock Forecasting\n",
    "\n",
    "This notebook demonstrates how to use Kolmogorov-Arnold Networks (KANs) for stock price prediction using an autoregressive approach.\n",
    "\n",
    "## Why KAN for Stock Prediction?\n",
    "\n",
    "KANs offer several advantages over traditional MLPs for time series:\n",
    "1. **Learnable activation functions**: KAN uses B-splines on edges, allowing the model to learn complex non-linear relationships\n",
    "2. **Better interpretability**: The learned functions can be visualized and understood\n",
    "3. **Handling non-smooth functions**: Stock data often has regime changes and discontinuities\n",
    "4. **Parameter efficiency**: KANs can achieve similar or better performance with fewer parameters\n",
    "\n",
    "## Approach\n",
    "\n",
    "We'll implement an autoregressive model where:\n",
    "- Input: Past N days of returns (or prices)\n",
    "- Output: Next day's return (or price)\n",
    "- Model: KAN network that learns the non-linear relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install pykan torch numpy pandas matplotlib requests scikit-learn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from kan import KAN, create_dataset\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Fetch Stock Data from EODHD\n",
    "\n",
    "EODHD (End of Day Historical Data) provides financial data APIs.\n",
    "You'll need an API key from: https://eodhistoricaldata.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "EODHD_API_KEY = \"YOUR_API_KEY_HERE\"  # Replace with your EODHD API key\n",
    "TICKER = \"AAPL.US\"  # Stock ticker (format: SYMBOL.EXCHANGE)\n",
    "START_DATE = \"2020-01-01\"\n",
    "END_DATE = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "def fetch_stock_data(ticker, start_date, end_date, api_key):\n",
    "    \"\"\"\n",
    "    Fetch stock data from EODHD API\n",
    "    \n",
    "    Args:\n",
    "        ticker: Stock symbol with exchange (e.g., 'AAPL.US')\n",
    "        start_date: Start date in 'YYYY-MM-DD' format\n",
    "        end_date: End date in 'YYYY-MM-DD' format\n",
    "        api_key: EODHD API key\n",
    "    \n",
    "    Returns:\n",
    "        pandas DataFrame with stock data\n",
    "    \"\"\"\n",
    "    url = f\"https://eodhistoricaldata.com/api/eod/{ticker}\"\n",
    "    params = {\n",
    "        'api_token': api_key,\n",
    "        'from': start_date,\n",
    "        'to': end_date,\n",
    "        'fmt': 'json'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        if not data:\n",
    "            raise ValueError(\"No data returned from API\")\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        df = df.sort_values('date').reset_index(drop=True)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching data: {e}\")\n",
    "        print(\"\\nFalling back to synthetic data for demonstration...\")\n",
    "        return generate_synthetic_data(start_date, end_date)\n",
    "\n",
    "def generate_synthetic_data(start_date, end_date):\n",
    "    \"\"\"\n",
    "    Generate synthetic stock data for testing without API key\n",
    "    \"\"\"\n",
    "    dates = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    n = len(dates)\n",
    "    \n",
    "    # Generate synthetic price with trend and volatility\n",
    "    np.random.seed(42)\n",
    "    trend = np.linspace(100, 150, n)\n",
    "    seasonal = 10 * np.sin(np.linspace(0, 8*np.pi, n))\n",
    "    noise = np.random.randn(n).cumsum() * 2\n",
    "    close = trend + seasonal + noise\n",
    "    \n",
    "    # Generate OHLV data based on close\n",
    "    high = close + np.abs(np.random.randn(n)) * 2\n",
    "    low = close - np.abs(np.random.randn(n)) * 2\n",
    "    open_price = close + np.random.randn(n) * 1.5\n",
    "    volume = np.random.randint(1000000, 10000000, n)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'date': dates,\n",
    "        'open': open_price,\n",
    "        'high': high,\n",
    "        'low': low,\n",
    "        'close': close,\n",
    "        'volume': volume\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Fetch data\n",
    "print(f\"Fetching data for {TICKER} from {START_DATE} to {END_DATE}...\")\n",
    "stock_data = fetch_stock_data(TICKER, START_DATE, END_DATE, EODHD_API_KEY)\n",
    "\n",
    "print(f\"\\nData shape: {stock_data.shape}\")\n",
    "print(f\"Date range: {stock_data['date'].min()} to {stock_data['date'].max()}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "stock_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the raw data\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "# Price plot\n",
    "axes[0].plot(stock_data['date'], stock_data['close'], label='Close Price', linewidth=1.5)\n",
    "axes[0].set_title(f'{TICKER} - Closing Price', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Price ($)', fontsize=12)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Volume plot\n",
    "axes[1].bar(stock_data['date'], stock_data['volume'], alpha=0.7, color='steelblue')\n",
    "axes[1].set_title('Trading Volume', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Date', fontsize=12)\n",
    "axes[1].set_ylabel('Volume', fontsize=12)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering for Autoregressive Model\n",
    "\n",
    "We'll create features based on:\n",
    "- Past returns (log returns)\n",
    "- Moving averages\n",
    "- Volatility measures\n",
    "- Technical indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df, lookback_window=10):\n",
    "    \"\"\"\n",
    "    Create autoregressive features for stock prediction\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with stock data\n",
    "        lookback_window: Number of past days to use as features\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with features\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Calculate returns\n",
    "    df['returns'] = np.log(df['close'] / df['close'].shift(1))\n",
    "    \n",
    "    # Lagged returns (main AR features)\n",
    "    for i in range(1, lookback_window + 1):\n",
    "        df[f'return_lag_{i}'] = df['returns'].shift(i)\n",
    "    \n",
    "    # Moving averages of returns\n",
    "    df['ma_5'] = df['returns'].rolling(window=5).mean()\n",
    "    df['ma_10'] = df['returns'].rolling(window=10).mean()\n",
    "    \n",
    "    # Volatility (rolling standard deviation)\n",
    "    df['volatility_5'] = df['returns'].rolling(window=5).std()\n",
    "    df['volatility_10'] = df['returns'].rolling(window=10).std()\n",
    "    \n",
    "    # Price momentum\n",
    "    df['momentum_3'] = df['close'] / df['close'].shift(3) - 1\n",
    "    df['momentum_5'] = df['close'] / df['close'].shift(5) - 1\n",
    "    \n",
    "    # Target: next day's return\n",
    "    df['target'] = df['returns'].shift(-1)\n",
    "    \n",
    "    # Drop rows with NaN values\n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create features\n",
    "LOOKBACK_WINDOW = 10\n",
    "feature_data = create_features(stock_data, LOOKBACK_WINDOW)\n",
    "\n",
    "print(f\"Feature data shape: {feature_data.shape}\")\n",
    "print(f\"\\nFeature columns:\")\n",
    "print([col for col in feature_data.columns if col not in ['date', 'open', 'high', 'low', 'close', 'volume']])\n",
    "print(\"\\nFirst few rows of features:\")\n",
    "feature_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Data for KAN Model\n",
    "\n",
    "We'll split the data into training, validation, and test sets, and convert to PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_kan(df, lookback_window=10, train_ratio=0.7, val_ratio=0.15):\n",
    "    \"\"\"\n",
    "    Prepare data for KAN training\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with features\n",
    "        lookback_window: Number of lagged returns to use\n",
    "        train_ratio: Proportion of data for training\n",
    "        val_ratio: Proportion of data for validation\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with train/val/test datasets and scaler\n",
    "    \"\"\"\n",
    "    # Select features: lagged returns and additional features\n",
    "    feature_cols = [f'return_lag_{i}' for i in range(1, lookback_window + 1)]\n",
    "    feature_cols += ['ma_5', 'ma_10', 'volatility_5', 'volatility_10', \n",
    "                     'momentum_3', 'momentum_5']\n",
    "    \n",
    "    X = df[feature_cols].values\n",
    "    y = df['target'].values.reshape(-1, 1)\n",
    "    \n",
    "    # Split data chronologically\n",
    "    n = len(X)\n",
    "    train_size = int(n * train_ratio)\n",
    "    val_size = int(n * val_ratio)\n",
    "    \n",
    "    X_train = X[:train_size]\n",
    "    y_train = y[:train_size]\n",
    "    \n",
    "    X_val = X[train_size:train_size + val_size]\n",
    "    y_val = y[train_size:train_size + val_size]\n",
    "    \n",
    "    X_test = X[train_size + val_size:]\n",
    "    y_test = y[train_size + val_size:]\n",
    "    \n",
    "    # Normalize features\n",
    "    scaler_X = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "    \n",
    "    X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "    X_val_scaled = scaler_X.transform(X_val)\n",
    "    X_test_scaled = scaler_X.transform(X_test)\n",
    "    \n",
    "    y_train_scaled = scaler_y.fit_transform(y_train)\n",
    "    y_val_scaled = scaler_y.transform(y_val)\n",
    "    y_test_scaled = scaler_y.transform(y_test)\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    dataset_train = {\n",
    "        'train_input': torch.FloatTensor(X_train_scaled),\n",
    "        'train_label': torch.FloatTensor(y_train_scaled),\n",
    "        'test_input': torch.FloatTensor(X_val_scaled),\n",
    "        'test_label': torch.FloatTensor(y_val_scaled)\n",
    "    }\n",
    "    \n",
    "    dataset_test = {\n",
    "        'input': torch.FloatTensor(X_test_scaled),\n",
    "        'label': torch.FloatTensor(y_test_scaled)\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'dataset_train': dataset_train,\n",
    "        'dataset_test': dataset_test,\n",
    "        'scaler_X': scaler_X,\n",
    "        'scaler_y': scaler_y,\n",
    "        'feature_cols': feature_cols,\n",
    "        'train_size': train_size,\n",
    "        'val_size': val_size,\n",
    "        'test_size': len(X_test),\n",
    "        'y_train': y_train,\n",
    "        'y_val': y_val,\n",
    "        'y_test': y_test\n",
    "    }\n",
    "\n",
    "# Prepare data\n",
    "data_dict = prepare_data_for_kan(feature_data, LOOKBACK_WINDOW)\n",
    "\n",
    "print(\"Data preparation complete!\")\n",
    "print(f\"\\nNumber of features: {len(data_dict['feature_cols'])}\")\n",
    "print(f\"Features: {data_dict['feature_cols']}\")\n",
    "print(f\"\\nTrain samples: {data_dict['train_size']}\")\n",
    "print(f\"Validation samples: {data_dict['val_size']}\")\n",
    "print(f\"Test samples: {data_dict['test_size']}\")\n",
    "print(f\"\\nInput shape: {data_dict['dataset_train']['train_input'].shape}\")\n",
    "print(f\"Output shape: {data_dict['dataset_train']['train_label'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build and Train KAN Model\n",
    "\n",
    "Now we'll create the KAN model with an appropriate architecture for our autoregressive task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KAN Model Configuration\n",
    "n_features = len(data_dict['feature_cols'])\n",
    "n_output = 1\n",
    "\n",
    "# Architecture: input -> hidden layers -> output\n",
    "# Start simple, can be expanded\n",
    "WIDTH = [n_features, 20, 10, n_output]  # [input_dim, hidden1, hidden2, output_dim]\n",
    "GRID = 5  # Grid size for B-splines\n",
    "K = 3  # Spline order (3 = cubic splines)\n",
    "\n",
    "print(f\"Creating KAN model with architecture: {WIDTH}\")\n",
    "print(f\"Grid size: {GRID}, Spline order: {K}\")\n",
    "\n",
    "# Create KAN model\n",
    "model = KAN(width=WIDTH, grid=GRID, k=K, seed=42)\n",
    "\n",
    "print(\"\\nModel created successfully!\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Training configuration\nTRAINING_STEPS = 100\nLAMB = 0.001  # L1 regularization for sparsity\nLAMB_ENTROPY = 2.0  # Entropy regularization\nOPTIMIZER = \"LBFGS\"  # LBFGS works well for small-to-medium datasets\n\nprint(\"Training KAN model...\")\nprint(f\"Steps: {TRAINING_STEPS}\")\nprint(f\"L1 regularization (lamb): {LAMB}\")\nprint(f\"Entropy regularization: {LAMB_ENTROPY}\")\nprint(f\"Optimizer: {OPTIMIZER}\")\nprint(\"\\nThis may take a few minutes...\\n\")\n\n# Train the model using fit() method\nresults = model.fit(\n    data_dict['dataset_train'],\n    opt=OPTIMIZER,\n    steps=TRAINING_STEPS,\n    lamb=LAMB,\n    lamb_entropy=LAMB_ENTROPY\n)\n\nprint(\"\\nTraining complete!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(results['train_loss'], label='Training Loss', linewidth=2)\n",
    "plt.plot(results['test_loss'], label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Steps', fontsize=12)\n",
    "plt.ylabel('Loss (MSE)', fontsize=12)\n",
    "plt.title('Training History', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(results['train_loss'], label='Training Loss (Linear)', linewidth=2)\n",
    "plt.plot(results['test_loss'], label='Validation Loss (Linear)', linewidth=2)\n",
    "plt.xlabel('Steps', fontsize=12)\n",
    "plt.ylabel('Loss (MSE)', fontsize=12)\n",
    "plt.title('Training History (Linear Scale)', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final Training Loss: {results['train_loss'][-1]:.6f}\")\n",
    "print(f\"Final Validation Loss: {results['test_loss'][-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation and Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataset, scaler_y, set_name=\"Test\"):\n",
    "    \"\"\"\n",
    "    Evaluate model performance\n",
    "    \n",
    "    Args:\n",
    "        model: Trained KAN model\n",
    "        dataset: Dictionary with 'input' and 'label' tensors\n",
    "        scaler_y: Scaler for inverse transforming predictions\n",
    "        set_name: Name of the dataset (for printing)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with predictions and metrics\n",
    "    \"\"\"\n",
    "    # Get predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions_scaled = model(dataset['input'])\n",
    "    \n",
    "    # Convert to numpy and inverse transform\n",
    "    predictions_scaled = predictions_scaled.numpy()\n",
    "    actuals_scaled = dataset['label'].numpy()\n",
    "    \n",
    "    predictions = scaler_y.inverse_transform(predictions_scaled)\n",
    "    actuals = scaler_y.inverse_transform(actuals_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(actuals, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(actuals, predictions)\n",
    "    r2 = r2_score(actuals, predictions)\n",
    "    \n",
    "    # Direction accuracy (for trading: did we predict the right direction?)\n",
    "    direction_actual = np.sign(actuals)\n",
    "    direction_pred = np.sign(predictions)\n",
    "    direction_accuracy = np.mean(direction_actual == direction_pred)\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"{set_name} Set Performance\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"MSE:                 {mse:.8f}\")\n",
    "    print(f\"RMSE:                {rmse:.8f}\")\n",
    "    print(f\"MAE:                 {mae:.8f}\")\n",
    "    print(f\"R² Score:            {r2:.4f}\")\n",
    "    print(f\"Direction Accuracy:  {direction_accuracy:.2%}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    return {\n",
    "        'predictions': predictions,\n",
    "        'actuals': actuals,\n",
    "        'mse': mse,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2,\n",
    "        'direction_accuracy': direction_accuracy\n",
    "    }\n",
    "\n",
    "# Evaluate on test set\n",
    "test_results = evaluate_model(\n",
    "    model, \n",
    "    data_dict['dataset_test'], \n",
    "    data_dict['scaler_y'],\n",
    "    set_name=\"Test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions vs actuals\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# Plot 1: Time series of predictions vs actuals\n",
    "axes[0, 0].plot(test_results['actuals'], label='Actual Returns', alpha=0.7, linewidth=1.5)\n",
    "axes[0, 0].plot(test_results['predictions'], label='Predicted Returns', alpha=0.7, linewidth=1.5)\n",
    "axes[0, 0].set_title('Predicted vs Actual Returns (Test Set)', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Sample', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Return', fontsize=12)\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Scatter plot\n",
    "axes[0, 1].scatter(test_results['actuals'], test_results['predictions'], alpha=0.5)\n",
    "min_val = min(test_results['actuals'].min(), test_results['predictions'].min())\n",
    "max_val = max(test_results['actuals'].max(), test_results['predictions'].max())\n",
    "axes[0, 1].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[0, 1].set_title('Prediction Scatter Plot', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Actual Returns', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Predicted Returns', fontsize=12)\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Prediction errors\n",
    "errors = test_results['predictions'].flatten() - test_results['actuals'].flatten()\n",
    "axes[1, 0].hist(errors, bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[1, 0].axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[1, 0].set_title('Distribution of Prediction Errors', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Error (Predicted - Actual)', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Cumulative returns\n",
    "cumulative_actual = np.cumsum(test_results['actuals'].flatten())\n",
    "cumulative_pred = np.cumsum(test_results['predictions'].flatten())\n",
    "axes[1, 1].plot(cumulative_actual, label='Actual Cumulative Return', linewidth=2)\n",
    "axes[1, 1].plot(cumulative_pred, label='Predicted Cumulative Return', linewidth=2)\n",
    "axes[1, 1].set_title('Cumulative Returns', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Sample', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Cumulative Return', fontsize=12)\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize KAN Network\n",
    "\n",
    "One of the key advantages of KAN is interpretability. Let's visualize the learned network structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the KAN network\n",
    "try:\n",
    "    model.plot(beta=3)\n",
    "    plt.title('KAN Network Structure', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"\\nThe network visualization shows the learned activation functions on each edge.\")\n",
    "    print(\"Thicker edges indicate more important connections.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not plot network: {e}\")\n",
    "    print(\"This is normal if the plot function requires additional dependencies.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_importance(model, dataset, feature_names):\n",
    "    \"\"\"\n",
    "    Analyze feature importance by measuring prediction change when features are perturbed\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get baseline predictions\n",
    "    with torch.no_grad():\n",
    "        baseline_pred = model(dataset['input']).numpy()\n",
    "    \n",
    "    importance_scores = []\n",
    "    \n",
    "    # Perturb each feature\n",
    "    for i in range(dataset['input'].shape[1]):\n",
    "        perturbed_input = dataset['input'].clone()\n",
    "        # Shuffle this feature across samples\n",
    "        perturbed_input[:, i] = perturbed_input[torch.randperm(perturbed_input.shape[0]), i]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            perturbed_pred = model(perturbed_input).numpy()\n",
    "        \n",
    "        # Calculate change in predictions\n",
    "        importance = np.mean(np.abs(perturbed_pred - baseline_pred))\n",
    "        importance_scores.append(importance)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importance_scores\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "# Calculate feature importance\n",
    "importance_df = analyze_feature_importance(\n",
    "    model, \n",
    "    data_dict['dataset_test'],\n",
    "    data_dict['feature_cols']\n",
    ")\n",
    "\n",
    "print(\"\\nFeature Importance (Test Set):\")\n",
    "print(importance_df.to_string(index=False))\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
    "plt.xlabel('Importance Score', fontsize=12)\n",
    "plt.ylabel('Feature', fontsize=12)\n",
    "plt.title('Feature Importance Analysis', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comparison with Baseline (Optional)\n",
    "\n",
    "Let's compare KAN with a simple linear regression baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Train linear regression baseline\n",
    "lr_model = LinearRegression()\n",
    "X_train = data_dict['dataset_train']['train_input'].numpy()\n",
    "y_train = data_dict['dataset_train']['train_label'].numpy()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "X_test = data_dict['dataset_test']['input'].numpy()\n",
    "y_test = data_dict['dataset_test']['label'].numpy()\n",
    "lr_pred_scaled = lr_model.predict(X_test)\n",
    "\n",
    "# Inverse transform\n",
    "lr_pred = data_dict['scaler_y'].inverse_transform(lr_pred_scaled)\n",
    "y_test_actual = data_dict['scaler_y'].inverse_transform(y_test)\n",
    "\n",
    "# Calculate metrics\n",
    "lr_mse = mean_squared_error(y_test_actual, lr_pred)\n",
    "lr_rmse = np.sqrt(lr_mse)\n",
    "lr_mae = mean_absolute_error(y_test_actual, lr_pred)\n",
    "lr_r2 = r2_score(y_test_actual, lr_pred)\n",
    "lr_direction_accuracy = np.mean(np.sign(y_test_actual) == np.sign(lr_pred))\n",
    "\n",
    "# Comparison table\n",
    "comparison = pd.DataFrame({\n",
    "    'Metric': ['MSE', 'RMSE', 'MAE', 'R²', 'Direction Accuracy'],\n",
    "    'KAN': [\n",
    "        test_results['mse'],\n",
    "        test_results['rmse'],\n",
    "        test_results['mae'],\n",
    "        test_results['r2'],\n",
    "        test_results['direction_accuracy']\n",
    "    ],\n",
    "    'Linear Regression': [\n",
    "        lr_mse,\n",
    "        lr_rmse,\n",
    "        lr_mae,\n",
    "        lr_r2,\n",
    "        lr_direction_accuracy\n",
    "    ]\n",
    "})\n",
    "\n",
    "comparison['Improvement'] = ((comparison['Linear Regression'] - comparison['KAN']) / \n",
    "                             comparison['Linear Regression'] * 100)\n",
    "comparison.loc[comparison['Metric'] == 'R²', 'Improvement'] = (\n",
    "    (comparison.loc[comparison['Metric'] == 'R²', 'KAN'].values[0] - \n",
    "     comparison.loc[comparison['Metric'] == 'R²', 'Linear Regression'].values[0]) / \n",
    "    abs(comparison.loc[comparison['Metric'] == 'R²', 'Linear Regression'].values[0]) * 100\n",
    ")\n",
    "comparison.loc[comparison['Metric'] == 'Direction Accuracy', 'Improvement'] = (\n",
    "    (comparison.loc[comparison['Metric'] == 'Direction Accuracy', 'KAN'].values[0] - \n",
    "     comparison.loc[comparison['Metric'] == 'Direction Accuracy', 'Linear Regression'].values[0]) / \n",
    "    comparison.loc[comparison['Metric'] == 'Direction Accuracy', 'Linear Regression'].values[0] * 100\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KAN vs Linear Regression Comparison\")\n",
    "print(\"=\"*70)\n",
    "print(comparison.to_string(index=False))\n",
    "print(\"\\nNote: Positive improvement means KAN performs better\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Next Steps\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **KAN for Stock Prediction**: We successfully applied KAN to autoregressive stock forecasting\n",
    "2. **Advantages**:\n",
    "   - Learnable activation functions adapt to data patterns\n",
    "   - Better interpretability through network visualization\n",
    "   - Can capture complex non-linear relationships\n",
    "3. **Challenges**:\n",
    "   - Training is slower than MLPs\n",
    "   - Requires careful hyperparameter tuning\n",
    "   - More parameters to configure (grid size, spline order, etc.)\n",
    "\n",
    "### Potential Improvements\n",
    "\n",
    "1. **Architecture tuning**:\n",
    "   - Experiment with different layer sizes\n",
    "   - Try different grid sizes and spline orders\n",
    "   - Add/remove hidden layers\n",
    "\n",
    "2. **Feature engineering**:\n",
    "   - Add more technical indicators (RSI, MACD, Bollinger Bands)\n",
    "   - Include market sentiment data\n",
    "   - Add macro-economic features\n",
    "\n",
    "3. **Training optimization**:\n",
    "   - Try different optimizers (Adam, SGD)\n",
    "   - Implement learning rate scheduling\n",
    "   - Use early stopping\n",
    "\n",
    "4. **Multi-step forecasting**:\n",
    "   - Predict multiple days ahead\n",
    "   - Implement recursive forecasting\n",
    "\n",
    "5. **Portfolio optimization**:\n",
    "   - Extend to multiple stocks\n",
    "   - Implement trading strategy based on predictions\n",
    "   - Backtest with realistic transaction costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "import pickle\n",
    "\n",
    "model_save_path = 'kan_stock_ar_model.pkl'\n",
    "\n",
    "with open(model_save_path, 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'model_state': model.state_dict(),\n",
    "        'model_config': {\n",
    "            'width': WIDTH,\n",
    "            'grid': GRID,\n",
    "            'k': K\n",
    "        },\n",
    "        'scaler_X': data_dict['scaler_X'],\n",
    "        'scaler_y': data_dict['scaler_y'],\n",
    "        'feature_cols': data_dict['feature_cols'],\n",
    "        'lookback_window': LOOKBACK_WINDOW\n",
    "    }, f)\n",
    "\n",
    "print(f\"Model saved to: {model_save_path}\")\n",
    "print(\"\\nTo load the model later, use:\")\n",
    "print(\"\"\"with open('kan_stock_ar_model.pkl', 'rb') as f:\n",
    "    checkpoint = pickle.load(f)\n",
    "    model = KAN(width=checkpoint['model_config']['width'], \n",
    "                grid=checkpoint['model_config']['grid'],\n",
    "                k=checkpoint['model_config']['k'])\n",
    "    model.load_state_dict(checkpoint['model_state'])\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}